{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a817075b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import time\n",
    "from typing import Dict, List, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67ea8cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HuggingFaceAPIDownloader:\n",
    "    def __init__(self, dataset_name=\"ithieund/VietNews-Abs-Sum\", data_dir=\"./data\"):\n",
    "        self.dataset_name = dataset_name\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.raw_dir = self.data_dir / \"raw\"\n",
    "        self.processed_dir = self.data_dir / \"processed\"\n",
    "        \n",
    "        self.raw_dir.mkdir(parents=True, exist_ok=True)\n",
    "        self.processed_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # API endpoints\n",
    "        self.base_url = \"https://datasets-server.huggingface.co\"\n",
    "        self.dataset_url = dataset_name.replace(\"/\", \"%2F\")\n",
    "        \n",
    "    def get_dataset_info(self) -> Optional[Dict]:\n",
    "        try:\n",
    "            url = f\"{self.base_url}/info?dataset={self.dataset_url}\"\n",
    "            print(f\"Fetching dataset info...\")\n",
    "            \n",
    "            response = requests.get(url, timeout=30)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            info = response.json()\n",
    "            \n",
    "            info_path = self.raw_dir / \"dataset_info.json\"\n",
    "            with open(info_path, 'w', encoding='utf-8') as f:\n",
    "                json.dump(info, f, indent=2, ensure_ascii=False)\n",
    "            \n",
    "            print(\"Dataset info retrieved\")\n",
    "            return info\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error getting dataset info: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def get_dataset_splits(self) -> List[str]:\n",
    "        try:\n",
    "            url = f\"{self.base_url}/splits?dataset={self.dataset_url}\"\n",
    "            print(\"Fetching available splits...\")\n",
    "            \n",
    "            response = requests.get(url, timeout=30)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            splits_info = response.json()\n",
    "            splits = [split['split'] for split in splits_info['splits']]\n",
    "            \n",
    "            print(f\"Available splits: {splits}\")\n",
    "            return splits\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error getting splits: {e}\")\n",
    "            return ['train', 'test']  # fallback\n",
    "    \n",
    "    def get_first_rows(self, split=\"train\", length=100) -> Optional[Dict]:\n",
    "        try:\n",
    "            url = f\"{self.base_url}/first-rows\"\n",
    "            params = {\n",
    "                'dataset': self.dataset_name,\n",
    "                'config': 'default',\n",
    "                'split': split,\n",
    "                'length': length\n",
    "            }\n",
    "            print(f\"Fetching first {length} rows from {split} split...\")\n",
    "            \n",
    "            response = requests.get(url, params=params, timeout=60)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            data = response.json()\n",
    "            print(f\"Retrieved {len(data['rows'])} rows from {split}\")\n",
    "            return data\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error getting first rows for {split}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def download_all_data_paginated(self, split=\"train\", batch_size=100, max_rows=None):\n",
    "        try:\n",
    "            print(f\"Starting paginated download for {split} split...\")\n",
    "            all_rows = []\n",
    "            offset = 0\n",
    "            \n",
    "            while True:\n",
    "                url = f\"{self.base_url}/rows\"\n",
    "                params = {\n",
    "                    'dataset': self.dataset_name,\n",
    "                    'config': 'default',\n",
    "                    'split': split,\n",
    "                    'offset': offset,\n",
    "                    'length': batch_size\n",
    "                }\n",
    "                print(f\"Fetching rows {offset} to {offset + batch_size}...\")\n",
    "                response = requests.get(url, params=params, timeout=60)\n",
    "                \n",
    "                if response.status_code == 404:\n",
    "                    print(\"No more data available\")\n",
    "                    break\n",
    "                    \n",
    "                response.raise_for_status()\n",
    "                data = response.json()\n",
    "                \n",
    "                batch_rows = data['rows']\n",
    "                if not batch_rows:\n",
    "                    print(\"No more rows to fetch\")\n",
    "                    break\n",
    "                \n",
    "                all_rows.extend(batch_rows)\n",
    "                print(f\"Total rows collected: {len(all_rows)}\")\n",
    "                \n",
    "                if max_rows and len(all_rows) >= max_rows:\n",
    "                    all_rows = all_rows[:max_rows]\n",
    "                    print(f\"Reached maximum rows limit: {max_rows}\")\n",
    "                    break\n",
    "                \n",
    "                offset += batch_size\n",
    "                time.sleep(1)  # Be nice to the API\n",
    "                \n",
    "                # Break if we got less than batch_size (likely the end)\n",
    "                if len(batch_rows) < batch_size:\n",
    "                    break\n",
    "            \n",
    "            return all_rows\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error during paginated download: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def process_and_save_data(self, rows: List[Dict], split: str):\n",
    "        if not rows:\n",
    "            print(f\"No data to save for {split}\")\n",
    "            return\n",
    "        processed_rows = []\n",
    "        for row in rows:\n",
    "            if 'row' in row:\n",
    "                processed_rows.append(row['row'])\n",
    "            else:\n",
    "                processed_rows.append(row)\n",
    "        \n",
    "        # Save as JSON\n",
    "        json_path = self.raw_dir / f\"{split}.json\"\n",
    "        with open(json_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(processed_rows, f, ensure_ascii=False, indent=2)\n",
    "        print(f\"Saved {len(processed_rows)} rows to {json_path}\")\n",
    "        \n",
    "        # Save as CSV\n",
    "        try:\n",
    "            df = pd.DataFrame(processed_rows)\n",
    "            csv_path = self.raw_dir / f\"{split}.csv\"\n",
    "            df.to_csv(csv_path, index=False, encoding='utf-8')\n",
    "            print(f\"Saved CSV to {csv_path}\")\n",
    "            \n",
    "            # Show basic info\n",
    "            print(f\"{split.upper()} Split Info:\")\n",
    "            print(f\"Rows: {len(df)}\")\n",
    "            print(f\"Columns: {list(df.columns)}\")\n",
    "            \n",
    "            # Show sample\n",
    "            if len(df) > 0:\n",
    "                print(f\"Sample data:\")\n",
    "                for col in df.columns:\n",
    "                    sample_val = str(df[col].iloc[0])\n",
    "                    preview = sample_val[:100] + \"...\" if len(sample_val) > 100 else sample_val\n",
    "                    print(f\"     {col}: {preview}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Could not create CSV: {e}\")\n",
    "    \n",
    "    def download_complete_dataset(self, max_rows_per_split=None):\n",
    "        print(\"üáªüá≥ VietNews Dataset Download via HuggingFace API\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        print(\"\\n1. Getting dataset information...\")\n",
    "        dataset_info = self.get_dataset_info()\n",
    "        \n",
    "        print(\"\\n2. Getting available splits...\")\n",
    "        splits = self.get_dataset_splits()\n",
    "        \n",
    "        print(\"\\n3. Downloading data splits...\")\n",
    "        for split in splits:\n",
    "            print(f\"\\n--- Downloading {split.upper()} split ---\")\n",
    "            \n",
    "            try:\n",
    "                rows = self.download_all_data_paginated(\n",
    "                    split=split, \n",
    "                    batch_size=100, \n",
    "                    max_rows=max_rows_per_split\n",
    "                )\n",
    "                \n",
    "                if rows:\n",
    "                    self.process_and_save_data(rows, split)\n",
    "                else:\n",
    "                    # Fallback to first-rows if full download fails\n",
    "                    print(f\"Falling back to first-rows for {split}...\")\n",
    "                    data = self.get_first_rows(split=split, length=100)\n",
    "                    if data:\n",
    "                        self.process_and_save_data(data['rows'], split)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error downloading {split}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        print(\"\\n4. Download Summary:\")\n",
    "        print(\"-\" * 30)\n",
    "        \n",
    "        files = list(self.raw_dir.glob(\"*\"))\n",
    "        for file_path in files:\n",
    "            print(f\"{file_path.name} ({file_path.stat().st_size / 1024:.1f} KB)\")\n",
    "        \n",
    "        print(f\"\\nDownload completed!\")\n",
    "        print(f\"Data location: {self.data_dir}\")\n",
    "    \n",
    "    def quick_test_api(self):\n",
    "        print(\"Testing HuggingFace API...\")\n",
    "        try:\n",
    "            url = f\"{self.base_url}/first-rows\"\n",
    "            params = {\n",
    "                'dataset': self.dataset_name,\n",
    "                'config': 'default',\n",
    "                'split': 'train',\n",
    "                'length': 5\n",
    "            }\n",
    "            \n",
    "            response = requests.get(url, params=params, timeout=30)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            data = response.json()\n",
    "            print(f\"API working! Got {len(data['rows'])} sample rows\")\n",
    "            \n",
    "            # Show structure\n",
    "            if data['rows']:\n",
    "                sample_row = data['rows'][0]\n",
    "                if 'row' in sample_row:\n",
    "                    print(f\"Sample columns: {list(sample_row['row'].keys())}\")\n",
    "                else:\n",
    "                    print(f\"Sample columns: {list(sample_row.keys())}\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"API test failed: {e}\")\n",
    "            return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7613dea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quick_download():\n",
    "    downloader = HuggingFaceAPIDownloader()\n",
    "    if downloader.quick_test_api():\n",
    "        downloader.download_complete_dataset(max_rows_per_split=1000)  # Limit for testing\n",
    "    else:\n",
    "        print(\"API not accessible. Please check your internet connection.\")\n",
    "\n",
    "def full_download():\n",
    "    downloader = HuggingFaceAPIDownloader()\n",
    "    downloader.download_complete_dataset()  # No limits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "648eebd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Choose download option:\n",
      "1. Quick test (1000 rows per split)\n",
      "2. Full download (all data)\n",
      "Testing HuggingFace API...\n",
      "API working! Got 62 sample rows\n",
      "Sample columns: ['guid', 'title', 'abstract', 'article']\n",
      "üáªüá≥ VietNews Dataset Download via HuggingFace API\n",
      "============================================================\n",
      "\n",
      "1. Getting dataset information...\n",
      "Fetching dataset info...\n",
      "Dataset info retrieved\n",
      "\n",
      "2. Getting available splits...\n",
      "Fetching available splits...\n",
      "Available splits: ['train', 'validation', 'test']\n",
      "\n",
      "3. Downloading data splits...\n",
      "\n",
      "--- Downloading TRAIN split ---\n",
      "Starting paginated download for train split...\n",
      "Fetching rows 0 to 100...\n",
      "Total rows collected: 100\n",
      "Fetching rows 100 to 200...\n",
      "Total rows collected: 200\n",
      "Fetching rows 200 to 300...\n",
      "Total rows collected: 300\n",
      "Fetching rows 300 to 400...\n",
      "Total rows collected: 400\n",
      "Fetching rows 400 to 500...\n",
      "Total rows collected: 500\n",
      "Fetching rows 500 to 600...\n",
      "Total rows collected: 600\n",
      "Fetching rows 600 to 700...\n",
      "Total rows collected: 700\n",
      "Fetching rows 700 to 800...\n",
      "Total rows collected: 800\n",
      "Fetching rows 800 to 900...\n",
      "Total rows collected: 900\n",
      "Fetching rows 900 to 1000...\n",
      "Total rows collected: 1000\n",
      "Reached maximum rows limit: 1000\n",
      "Saved 1000 rows to data\\raw\\train.json\n",
      "Saved CSV to data\\raw\\train.csv\n",
      "TRAIN Split Info:\n",
      "Rows: 1000\n",
      "Columns: ['guid', 'title', 'abstract', 'article']\n",
      "Sample data:\n",
      "     guid: 1\n",
      "     title: Kh·ªüi_t·ªë k·∫ª_tr·ªôm h∆°n 1 t·∫° th√≥c v√† h∆°n 8 tri·ªáu ƒë·ªìng c·ªßa ch√∫ ru·ªôt ƒë·ªÉ l·∫•y ti·ªÅn mua ma_tu√Ω\n",
      "     abstract: V·ªõi b·∫£n_t√≠nh ham ch∆°i , l∆∞·ªùi l√†m , c√≥ nhi·ªÅu ti·ªÅn_√°n ti·ªÅn_s·ª± , l·∫°i nghi·ªán ma_tu√Ω , Th∆∞∆°ng ƒë√£ ƒë·ªôt_nh·∫≠p...\n",
      "     article: Ng√†y 27/3 , C∆°_quan C·∫£nh_s√°t ƒëi·ªÅu_tra C√¥ng_an TP. H∆∞ng_Y√™n , t·ªânh H∆∞ng_Y√™n cho bi·∫øt , ƒë∆°n_v·ªã v·ª´a ra ...\n",
      "\n",
      "--- Downloading VALIDATION split ---\n",
      "Starting paginated download for validation split...\n",
      "Fetching rows 0 to 100...\n",
      "Total rows collected: 100\n",
      "Fetching rows 100 to 200...\n",
      "Total rows collected: 200\n",
      "Fetching rows 200 to 300...\n",
      "Total rows collected: 300\n",
      "Fetching rows 300 to 400...\n",
      "Total rows collected: 400\n",
      "Fetching rows 400 to 500...\n",
      "Total rows collected: 500\n",
      "Fetching rows 500 to 600...\n",
      "Total rows collected: 600\n",
      "Fetching rows 600 to 700...\n",
      "Total rows collected: 700\n",
      "Fetching rows 700 to 800...\n",
      "Total rows collected: 800\n",
      "Fetching rows 800 to 900...\n",
      "Total rows collected: 900\n",
      "Fetching rows 900 to 1000...\n",
      "Total rows collected: 1000\n",
      "Reached maximum rows limit: 1000\n",
      "Saved 1000 rows to data\\raw\\validation.json\n",
      "Saved CSV to data\\raw\\validation.csv\n",
      "VALIDATION Split Info:\n",
      "Rows: 1000\n",
      "Columns: ['guid', 'title', 'abstract', 'article']\n",
      "Sample data:\n",
      "     guid: 1\n",
      "     title: Kh·ªüi_t·ªë , t·∫°m giam hai nh√¢n_vi√™n c√¥ng_ty ƒê·ªãa_·ªëc Alibaba ch·ªëng l·ªánh c∆∞·ª°ng_ch·∫ø\n",
      "     abstract: Vi·ªán_ki·ªÉm_s√°t nh√¢n_d√¢n T X.Ph√∫ M·ªπ , t·ªânh B√†_R·ªãa ‚Äì V≈©ng_T√†u v·ª´a ph√™_chu·∫©n quy·∫øt_ƒë·ªãnh kh·ªüi_t·ªë b·ªã_can ,...\n",
      "     article: Ng√†y 22/6 , c∆°_quan CSƒêT C√¥ng_an T X.Ph√∫ M·ªπ ƒë√£ ra quy·∫øt_ƒë·ªãnh kh·ªüi_t·ªë v·ª• √°n h√¨nh_s·ª± v·ªÅ c√°c t·ªôi G√¢y_r·ªë...\n",
      "\n",
      "--- Downloading TEST split ---\n",
      "Starting paginated download for test split...\n",
      "Fetching rows 0 to 100...\n",
      "Total rows collected: 100\n",
      "Fetching rows 100 to 200...\n",
      "Total rows collected: 200\n",
      "Fetching rows 200 to 300...\n",
      "Total rows collected: 300\n",
      "Fetching rows 300 to 400...\n",
      "Total rows collected: 400\n",
      "Fetching rows 400 to 500...\n",
      "Total rows collected: 500\n",
      "Fetching rows 500 to 600...\n",
      "Total rows collected: 600\n",
      "Fetching rows 600 to 700...\n",
      "Total rows collected: 700\n",
      "Fetching rows 700 to 800...\n",
      "Total rows collected: 800\n",
      "Fetching rows 800 to 900...\n",
      "Total rows collected: 900\n",
      "Fetching rows 900 to 1000...\n",
      "Total rows collected: 1000\n",
      "Reached maximum rows limit: 1000\n",
      "Saved 1000 rows to data\\raw\\test.json\n",
      "Saved CSV to data\\raw\\test.csv\n",
      "TEST Split Info:\n",
      "Rows: 1000\n",
      "Columns: ['guid', 'title', 'abstract', 'article']\n",
      "Sample data:\n",
      "     guid: 1\n",
      "     title: B·∫£n_√°n cho ƒë·ªëi_t∆∞·ª£ng gi·∫£_danh c√¥ng_an ƒë·ªÉ l·ª´a_ƒë·∫£o\n",
      "     abstract: Ng√†y 25/2 , TAND TP. ƒê√†_N·∫µng tuy√™n_ph·∫°t H·ªì_Xu√¢n_Huy ( SN 1994 ) , ng·ª• qu·∫≠n H·∫£i_Ch√¢u , 12 nƒÉm t√π v·ªÅ t...\n",
      "     article: Theo l·ªùi khai c·ªßa Huy t·∫°i phi√™n_to√† , ƒë·ªÉ c√≥ ti·ªÅn s·ª≠_d·ª•ng c√°_nh√¢n , Huy \" n·ªï \" l√† sƒ©_quan c·ª•c Ph√≤ng_c...\n",
      "\n",
      "4. Download Summary:\n",
      "------------------------------\n",
      "dataset_info.json (3.3 KB)\n",
      "test.csv (3064.9 KB)\n",
      "test.json (3137.5 KB)\n",
      "train.csv (3065.0 KB)\n",
      "train.json (3137.5 KB)\n",
      "validation.csv (3084.6 KB)\n",
      "validation.json (3157.2 KB)\n",
      "\n",
      "Download completed!\n",
      "Data location: data\n"
     ]
    }
   ],
   "source": [
    "print(\"Choose download option:\")\n",
    "print(\"1. Quick test (1000 rows per split)\")\n",
    "print(\"2. Full download (all data)\")\n",
    "    \n",
    "choice = input(\"Enter choice (1 or 2): \").strip()\n",
    "    \n",
    "if choice == \"1\":\n",
    "    quick_download()\n",
    "elif choice == \"2\":\n",
    "    full_download()\n",
    "else:\n",
    "    print(\"Invalid choice. Running quick test...\")\n",
    "    quick_download()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
