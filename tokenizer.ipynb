{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9b9ed4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "import re\n",
    "import json\n",
    "import unicodedata\n",
    "from typing import List, Tuple, Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e92ca768",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(text: str) -> str:\n",
    "    return unicodedata.normalize('NFC', text).strip()\n",
    "\n",
    "\n",
    "# very simple pre-tokenizer: separate common punctuation from words\n",
    "_PUNCT_RE = re.compile(r\"([\\.,!\\?:;()\\[\\]\\\"'“”„—–])\")\n",
    "\n",
    "\n",
    "def pre_tokenize(text: str) -> List[str]:\n",
    "    text = normalize_text(text)\n",
    "    # space around punctuation\n",
    "    text = _PUNCT_RE.sub(r\" \\1 \", text)\n",
    "    # collapse multiple spaces\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text.split(' ')\n",
    "\n",
    "def get_vocabulary_from_corpus(corpus_lines: List[str]) -> Dict[Tuple[str, ...], int]:\n",
    "    vocab = defaultdict(int)\n",
    "    for line in corpus_lines:\n",
    "        tokens = pre_tokenize(line)\n",
    "        for tok in tokens:\n",
    "            if tok == '':\n",
    "                continue\n",
    "            word = list(tok) + ['</w>']\n",
    "            vocab[tuple(word)] += 1\n",
    "    return vocab\n",
    "\n",
    "def get_stats(vocab: Dict[Tuple[str, ...], int]) -> Dict[Tuple[str, str], int]:\n",
    "    pairs = defaultdict(int)\n",
    "    for word, freq in vocab.items():\n",
    "        symbols = list(word)\n",
    "        for i in range(len(symbols) - 1):\n",
    "            pairs[(symbols[i], symbols[i+1])] += freq\n",
    "    return pairs\n",
    "\n",
    "def merge_vocab(pair: Tuple[str, str], vocab: Dict[Tuple[str, ...], int]) -> Dict[Tuple[str, ...], int]:\n",
    "    \"\"\"Merge all occurrences of the given pair in the vocab and return new vocab.\"\"\"\n",
    "    merged_vocab = {}\n",
    "    bigram = re.escape(' '.join(pair))\n",
    "    pattern = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
    "    # We'll represent words as space-joined symbols for using regex replacement safely\n",
    "    for word, freq in vocab.items():\n",
    "        word_str = ' '.join(word)\n",
    "        # replace occurrences of the pair with merged symbol\n",
    "        new_word_str = pattern.sub(''.join(pair), word_str)\n",
    "        new_word = tuple(new_word_str.split(' '))\n",
    "        merged_vocab[new_word] = freq if new_word not in merged_vocab else merged_vocab[new_word] + freq\n",
    "    return merged_vocab\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train_bpe(corpus_lines: List[str], num_merges: int = 10000, min_freq: int = 2) -> Tuple[List[Tuple[str, str]], Dict[Tuple[str, ...], int]]:\n",
    "    \"\"\"Train BPE merges on the given corpus lines.\n",
    "\n",
    "\n",
    "    Args:\n",
    "    corpus_lines: list of raw text lines\n",
    "    num_merges: number of merge operations\n",
    "    min_freq: minimum frequency of a pair to consider merging\n",
    "\n",
    "\n",
    "    Returns:\n",
    "    merges: ordered list of merged pairs (tuple of two symbols)\n",
    "    vocab: final vocabulary mapping (tuple(symbols) -> frequency)\n",
    "    \"\"\"\n",
    "    vocab = get_vocabulary_from_corpus(corpus_lines)\n",
    "    merges: List[Tuple[str, str]] = []\n",
    "\n",
    "\n",
    "    for i in range(num_merges):\n",
    "        pairs = get_stats(vocab)\n",
    "        if not pairs:\n",
    "            break\n",
    "        # pick the most frequent pair\n",
    "        best_pair, best_freq = max(pairs.items(), key=lambda kv: kv[1])\n",
    "        if best_freq < min_freq:\n",
    "            break\n",
    "        merges.append(best_pair)\n",
    "        vocab = merge_vocab(best_pair, vocab)\n",
    "    return merges, vocab\n",
    "\n",
    "\n",
    "# ----------------------------- Encoding / Tokenizing -----------------------------\n",
    "\n",
    "\n",
    "def build_merge_lookup(merges: List[Tuple[str, str]]) -> Dict[Tuple[str, str], int]:\n",
    "    return {pair: idx for idx, pair in enumerate(merges)}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def encode_word(word: str, merges: List[Tuple[str, str]]) -> List[str]:\n",
    "    \"\"\"Encode a single word into BPE tokens using the learned merges.\n",
    "\n",
    "\n",
    "    We use an ordered-merge greedy algorithm: apply merges in the order they were learned.\n",
    "    \"\"\"\n",
    "    # initialize\n",
    "    symbols = list(word) + ['</w>']\n",
    "    merges_set = set(merges)\n",
    "\n",
    "\n",
    "    # We'll attempt to apply merges repeatedly in the learned order. This is simple but not the fastest.\n",
    "    merged = True\n",
    "    while merged:\n",
    "        merged = False\n",
    "        i = 0\n",
    "        new_symbols = []\n",
    "        while i < len(symbols):\n",
    "            if i < len(symbols) - 1 and (symbols[i], symbols[i+1]) in merges_set:\n",
    "                # merge this pair\n",
    "                new_symbols.append(symbols[i] + symbols[i+1])\n",
    "                i += 2\n",
    "                merged = True\n",
    "            else:\n",
    "                new_symbols.append(symbols[i])\n",
    "                i += 1\n",
    "        symbols = new_symbols\n",
    "    # remove end-of-word marker by concatenating with previous token or keeping it as separate indicator\n",
    "    if symbols and symbols[-1] == '</w>':\n",
    "        symbols = symbols[:-1]\n",
    "    return symbols\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def encode(text: str, merges: List[Tuple[str, str]]) -> List[str]:\n",
    "    \"\"\"Encode a full text into a sequence of BPE tokens (flattened).\"\"\"\n",
    "    tokens = []\n",
    "    for tok in pre_tokenize(text):\n",
    "        if tok == '':\n",
    "            continue\n",
    "        enc = encode_word(tok, merges)\n",
    "        tokens.extend(enc + [' ']) # append a space token as separator (optional)\n",
    "    # remove trailing space token\n",
    "    if tokens and tokens[-1] == ' ':\n",
    "        tokens = tokens[:-1]\n",
    "    return tokens\n",
    "\n",
    "\n",
    "# ----------------------------- Persistence -----------------------------\n",
    "\n",
    "\n",
    "def save_merges(merges: List[Tuple[str, str]], path: str):\n",
    "    with open(path, 'w', encoding='utf-8') as f:\n",
    "        json.dump([list(pair) for pair in merges], f, ensure_ascii=False, indent=2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def load_merges(path: str) -> List[Tuple[str, str]]:\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        raw = json.load(f)\n",
    "    return [tuple(pair) for pair in raw]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f5c4202",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training BPE on a tiny sample corpus (for demonstration)...\n",
      "Learned 21 merges (showing first 30):\n",
      "  1. n + </w>\n",
      "  2. i + </w>\n",
      "  3. . + </w>\n",
      "  4. t + h\n",
      "  5. h + </w>\n",
      "  6. ô + i</w>\n",
      "  7. à + </w>\n",
      "  8. T + ôi</w>\n",
      "  9. t + r\n",
      " 10. p + </w>\n",
      " 11. th + í\n",
      " 12. thí + c\n",
      " 13. thíc + h</w>\n",
      " 14. p + h\n",
      " 15. n + g\n",
      " 16. ng + </w>\n",
      " 17. n + h</w>\n",
      " 18. P + y\n",
      " 19. Py + th\n",
      " 20. Pyth + o\n",
      " 21. Pytho + n</w>\n",
      "\n",
      "Original: Tôi thích cà phê và Python.\n",
      "BPE tokens: ['Tôi</w>', ' ', 'thích</w>', ' ', 'c', 'à</w>', ' ', 'ph', 'ê', ' ', 'v', 'à</w>', ' ', 'Python</w>', ' ', '.</w>']\n",
      "\n",
      "Saved merges to bpe_tokenizer/bpe_merges.json\n"
     ]
    }
   ],
   "source": [
    "sample = [\n",
    "    'Xin chào! Tôi tên là An.',\n",
    "    'Hôm nay trời đẹp và tôi thích cà phê.',\n",
    "    'Bạn có muốn đi ăn phở không?',\n",
    "    'Tôi thích lập trình bằng Python. Python rất mạnh.'\n",
    "]\n",
    "\n",
    "\n",
    "print('Training BPE on a tiny sample corpus (for demonstration)...')\n",
    "merges, vocab = train_bpe(sample, num_merges=20000, min_freq=2)\n",
    "print(f'Learned {len(merges)} merges (showing first 30):')\n",
    "for i, m in enumerate(merges[:30]):\n",
    "    print(f'{i+1:3d}. {m[0]} + {m[1]}')\n",
    "\n",
    "\n",
    "test_sent = 'Tôi thích cà phê và Python.'\n",
    "toks = encode(test_sent, merges)\n",
    "print('\\nOriginal:', test_sent)\n",
    "print('BPE tokens:', toks)\n",
    "\n",
    "\n",
    "# Save merges for later reuse\n",
    "save_merges(merges, 'bpe_tokenizer/bpe_merges.json')\n",
    "print('\\nSaved merges to bpe_tokenizer/bpe_merges.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
