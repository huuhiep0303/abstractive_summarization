{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a817075b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import time\n",
    "from typing import Dict, List, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67ea8cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HuggingFaceAPIDownloader:\n",
    "    \"\"\"Download VietNews dataset using HuggingFace Datasets Server API\"\"\"\n",
    "    \n",
    "    def __init__(self, dataset_name=\"ithieund/VietNews-Abs-Sum\", data_dir=\"./data\"):\n",
    "        self.dataset_name = dataset_name\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.raw_dir = self.data_dir / \"raw\"\n",
    "        self.processed_dir = self.data_dir / \"processed\"\n",
    "        \n",
    "        # Create directories\n",
    "        self.raw_dir.mkdir(parents=True, exist_ok=True)\n",
    "        self.processed_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # API endpoints\n",
    "        self.base_url = \"https://datasets-server.huggingface.co\"\n",
    "        self.dataset_url = dataset_name.replace(\"/\", \"%2F\")\n",
    "        \n",
    "    def get_dataset_info(self) -> Optional[Dict]:\n",
    "        \"\"\"Get basic dataset information\"\"\"\n",
    "        try:\n",
    "            url = f\"{self.base_url}/info?dataset={self.dataset_url}\"\n",
    "            print(f\"Fetching dataset info...\")\n",
    "            \n",
    "            response = requests.get(url, timeout=30)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            info = response.json()\n",
    "            \n",
    "            # Save dataset info\n",
    "            info_path = self.raw_dir / \"dataset_info.json\"\n",
    "            with open(info_path, 'w', encoding='utf-8') as f:\n",
    "                json.dump(info, f, indent=2, ensure_ascii=False)\n",
    "            \n",
    "            print(\"‚úÖ Dataset info retrieved\")\n",
    "            return info\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error getting dataset info: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def get_dataset_splits(self) -> List[str]:\n",
    "        \"\"\"Get available dataset splits\"\"\"\n",
    "        try:\n",
    "            url = f\"{self.base_url}/splits?dataset={self.dataset_url}\"\n",
    "            print(\"Fetching available splits...\")\n",
    "            \n",
    "            response = requests.get(url, timeout=30)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            splits_info = response.json()\n",
    "            splits = [split['split'] for split in splits_info['splits']]\n",
    "            \n",
    "            print(f\"‚úÖ Available splits: {splits}\")\n",
    "            return splits\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error getting splits: {e}\")\n",
    "            return ['train', 'test']  # fallback\n",
    "    \n",
    "    def get_first_rows(self, split=\"train\", length=100) -> Optional[Dict]:\n",
    "        \"\"\"Get first rows from a specific split\"\"\"\n",
    "        try:\n",
    "            url = f\"{self.base_url}/first-rows\"\n",
    "            params = {\n",
    "                'dataset': self.dataset_name,\n",
    "                'config': 'default',\n",
    "                'split': split,\n",
    "                'length': length\n",
    "            }\n",
    "            \n",
    "            print(f\"Fetching first {length} rows from {split} split...\")\n",
    "            \n",
    "            response = requests.get(url, params=params, timeout=60)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            data = response.json()\n",
    "            print(f\"‚úÖ Retrieved {len(data['rows'])} rows from {split}\")\n",
    "            \n",
    "            return data\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error getting first rows for {split}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def download_all_data_paginated(self, split=\"train\", batch_size=100, max_rows=None):\n",
    "        \"\"\"Download data in batches using pagination\"\"\"\n",
    "        try:\n",
    "            print(f\"Starting paginated download for {split} split...\")\n",
    "            \n",
    "            all_rows = []\n",
    "            offset = 0\n",
    "            \n",
    "            while True:\n",
    "                url = f\"{self.base_url}/rows\"\n",
    "                params = {\n",
    "                    'dataset': self.dataset_name,\n",
    "                    'config': 'default',\n",
    "                    'split': split,\n",
    "                    'offset': offset,\n",
    "                    'length': batch_size\n",
    "                }\n",
    "                \n",
    "                print(f\"Fetching rows {offset} to {offset + batch_size}...\")\n",
    "                \n",
    "                response = requests.get(url, params=params, timeout=60)\n",
    "                \n",
    "                if response.status_code == 404:\n",
    "                    print(\"No more data available\")\n",
    "                    break\n",
    "                    \n",
    "                response.raise_for_status()\n",
    "                data = response.json()\n",
    "                \n",
    "                batch_rows = data['rows']\n",
    "                if not batch_rows:\n",
    "                    print(\"No more rows to fetch\")\n",
    "                    break\n",
    "                \n",
    "                all_rows.extend(batch_rows)\n",
    "                print(f\"Total rows collected: {len(all_rows)}\")\n",
    "                \n",
    "                # Check if we've reached max_rows limit\n",
    "                if max_rows and len(all_rows) >= max_rows:\n",
    "                    all_rows = all_rows[:max_rows]\n",
    "                    print(f\"Reached maximum rows limit: {max_rows}\")\n",
    "                    break\n",
    "                \n",
    "                offset += batch_size\n",
    "                time.sleep(1)  # Be nice to the API\n",
    "                \n",
    "                # Break if we got less than batch_size (likely the end)\n",
    "                if len(batch_rows) < batch_size:\n",
    "                    break\n",
    "            \n",
    "            return all_rows\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error during paginated download: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def process_and_save_data(self, rows: List[Dict], split: str):\n",
    "        \"\"\"Process and save the downloaded data\"\"\"\n",
    "        if not rows:\n",
    "            print(f\"No data to save for {split}\")\n",
    "            return\n",
    "        \n",
    "        # Extract just the row data\n",
    "        processed_rows = []\n",
    "        for row in rows:\n",
    "            if 'row' in row:\n",
    "                processed_rows.append(row['row'])\n",
    "            else:\n",
    "                processed_rows.append(row)\n",
    "        \n",
    "        # Save as JSON\n",
    "        json_path = self.raw_dir / f\"{split}.json\"\n",
    "        with open(json_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(processed_rows, f, ensure_ascii=False, indent=2)\n",
    "        print(f\"‚úÖ Saved {len(processed_rows)} rows to {json_path}\")\n",
    "        \n",
    "        # Save as CSV\n",
    "        try:\n",
    "            df = pd.DataFrame(processed_rows)\n",
    "            csv_path = self.raw_dir / f\"{split}.csv\"\n",
    "            df.to_csv(csv_path, index=False, encoding='utf-8')\n",
    "            print(f\"‚úÖ Saved CSV to {csv_path}\")\n",
    "            \n",
    "            # Show basic info\n",
    "            print(f\"üìä {split.upper()} Split Info:\")\n",
    "            print(f\"   - Rows: {len(df)}\")\n",
    "            print(f\"   - Columns: {list(df.columns)}\")\n",
    "            \n",
    "            # Show sample\n",
    "            if len(df) > 0:\n",
    "                print(f\"   - Sample data:\")\n",
    "                for col in df.columns:\n",
    "                    sample_val = str(df[col].iloc[0])\n",
    "                    preview = sample_val[:100] + \"...\" if len(sample_val) > 100 else sample_val\n",
    "                    print(f\"     {col}: {preview}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è  Could not create CSV: {e}\")\n",
    "    \n",
    "    def download_complete_dataset(self, max_rows_per_split=None):\n",
    "        \"\"\"Download complete dataset using API\"\"\"\n",
    "        print(\"üáªüá≥ VietNews Dataset Download via HuggingFace API\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Step 1: Get dataset info\n",
    "        print(\"\\n1. Getting dataset information...\")\n",
    "        dataset_info = self.get_dataset_info()\n",
    "        \n",
    "        # Step 2: Get available splits\n",
    "        print(\"\\n2. Getting available splits...\")\n",
    "        splits = self.get_dataset_splits()\n",
    "        \n",
    "        # Step 3: Download each split\n",
    "        print(\"\\n3. Downloading data splits...\")\n",
    "        for split in splits:\n",
    "            print(f\"\\n--- Downloading {split.upper()} split ---\")\n",
    "            \n",
    "            # Try full download first\n",
    "            try:\n",
    "                rows = self.download_all_data_paginated(\n",
    "                    split=split, \n",
    "                    batch_size=100, \n",
    "                    max_rows=max_rows_per_split\n",
    "                )\n",
    "                \n",
    "                if rows:\n",
    "                    self.process_and_save_data(rows, split)\n",
    "                else:\n",
    "                    # Fallback to first-rows if full download fails\n",
    "                    print(f\"Falling back to first-rows for {split}...\")\n",
    "                    data = self.get_first_rows(split=split, length=100)\n",
    "                    if data:\n",
    "                        self.process_and_save_data(data['rows'], split)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error downloading {split}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Step 4: Summary\n",
    "        print(\"\\n4. Download Summary:\")\n",
    "        print(\"-\" * 30)\n",
    "        \n",
    "        files = list(self.raw_dir.glob(\"*\"))\n",
    "        for file_path in files:\n",
    "            print(f\"üìÅ {file_path.name} ({file_path.stat().st_size / 1024:.1f} KB)\")\n",
    "        \n",
    "        print(f\"\\n‚úÖ Download completed!\")\n",
    "        print(f\"üìÇ Data location: {self.data_dir}\")\n",
    "    \n",
    "    def quick_test_api(self):\n",
    "        \"\"\"Quick test to see if API is working\"\"\"\n",
    "        print(\"üß™ Testing HuggingFace API...\")\n",
    "        \n",
    "        # Test with first-rows endpoint\n",
    "        try:\n",
    "            url = f\"{self.base_url}/first-rows\"\n",
    "            params = {\n",
    "                'dataset': self.dataset_name,\n",
    "                'config': 'default',\n",
    "                'split': 'train',\n",
    "                'length': 5\n",
    "            }\n",
    "            \n",
    "            response = requests.get(url, params=params, timeout=30)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            data = response.json()\n",
    "            print(f\"‚úÖ API working! Got {len(data['rows'])} sample rows\")\n",
    "            \n",
    "            # Show structure\n",
    "            if data['rows']:\n",
    "                sample_row = data['rows'][0]\n",
    "                if 'row' in sample_row:\n",
    "                    print(f\"üìù Sample columns: {list(sample_row['row'].keys())}\")\n",
    "                else:\n",
    "                    print(f\"üìù Sample columns: {list(sample_row.keys())}\")\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå API test failed: {e}\")\n",
    "            return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7613dea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quick_download():\n",
    "    \"\"\"Quick download with default settings\"\"\"\n",
    "    downloader = HuggingFaceAPIDownloader()\n",
    "    \n",
    "    # Test API first\n",
    "    if downloader.quick_test_api():\n",
    "        downloader.download_complete_dataset(max_rows_per_split=1000)  # Limit for testing\n",
    "    else:\n",
    "        print(\"API not accessible. Please check your internet connection.\")\n",
    "\n",
    "def full_download():\n",
    "    \"\"\"Full download without limits\"\"\"\n",
    "    downloader = HuggingFaceAPIDownloader()\n",
    "    downloader.download_complete_dataset()  # No limits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "648eebd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Choose download option:\n",
      "1. Quick test (1000 rows per split)\n",
      "2. Full download (all data)\n",
      "üáªüá≥ VietNews Dataset Download via HuggingFace API\n",
      "============================================================\n",
      "\n",
      "1. Getting dataset information...\n",
      "Fetching dataset info...\n",
      "‚úÖ Dataset info retrieved\n",
      "\n",
      "2. Getting available splits...\n",
      "Fetching available splits...\n",
      "‚úÖ Available splits: ['train', 'validation', 'test']\n",
      "\n",
      "3. Downloading data splits...\n",
      "\n",
      "--- Downloading TRAIN split ---\n",
      "Starting paginated download for train split...\n",
      "Fetching rows 0 to 100...\n",
      "Total rows collected: 100\n",
      "Fetching rows 100 to 200...\n",
      "Total rows collected: 200\n",
      "Fetching rows 200 to 300...\n",
      "Total rows collected: 300\n",
      "Fetching rows 300 to 400...\n",
      "Total rows collected: 400\n",
      "Fetching rows 400 to 500...\n",
      "Total rows collected: 500\n",
      "Fetching rows 500 to 600...\n",
      "Total rows collected: 600\n",
      "Fetching rows 600 to 700...\n",
      "Total rows collected: 700\n",
      "Fetching rows 700 to 800...\n",
      "Total rows collected: 800\n",
      "Fetching rows 800 to 900...\n",
      "Total rows collected: 900\n",
      "Fetching rows 900 to 1000...\n",
      "Total rows collected: 1000\n",
      "Fetching rows 1000 to 1100...\n",
      "Total rows collected: 1100\n",
      "Fetching rows 1100 to 1200...\n",
      "Total rows collected: 1200\n",
      "Fetching rows 1200 to 1300...\n",
      "Total rows collected: 1300\n",
      "Fetching rows 1300 to 1400...\n",
      "Total rows collected: 1400\n",
      "Fetching rows 1400 to 1500...\n",
      "Total rows collected: 1500\n",
      "Fetching rows 1500 to 1600...\n",
      "Total rows collected: 1600\n",
      "Fetching rows 1600 to 1700...\n",
      "Total rows collected: 1700\n",
      "Fetching rows 1700 to 1800...\n",
      "Total rows collected: 1800\n",
      "Fetching rows 1800 to 1900...\n",
      "Total rows collected: 1900\n",
      "Fetching rows 1900 to 2000...\n",
      "Total rows collected: 2000\n",
      "Fetching rows 2000 to 2100...\n",
      "Total rows collected: 2100\n",
      "Fetching rows 2100 to 2200...\n",
      "Total rows collected: 2200\n",
      "Fetching rows 2200 to 2300...\n",
      "Total rows collected: 2300\n",
      "Fetching rows 2300 to 2400...\n",
      "Total rows collected: 2400\n",
      "Fetching rows 2400 to 2500...\n",
      "Total rows collected: 2500\n",
      "Fetching rows 2500 to 2600...\n",
      "Total rows collected: 2600\n",
      "Fetching rows 2600 to 2700...\n",
      "Total rows collected: 2700\n",
      "Fetching rows 2700 to 2800...\n",
      "Total rows collected: 2800\n",
      "Fetching rows 2800 to 2900...\n",
      "Total rows collected: 2900\n",
      "Fetching rows 2900 to 3000...\n",
      "Total rows collected: 3000\n",
      "Fetching rows 3000 to 3100...\n",
      "Total rows collected: 3100\n",
      "Fetching rows 3100 to 3200...\n",
      "‚ùå Error during paginated download: 502 Server Error: Bad Gateway for url: https://datasets-server.huggingface.co/rows?dataset=ithieund%2FVietNews-Abs-Sum&config=default&split=train&offset=3100&length=100\n",
      "Falling back to first-rows for train...\n",
      "Fetching first 100 rows from train split...\n",
      "‚úÖ Retrieved 62 rows from train\n",
      "‚úÖ Saved 62 rows to data\\raw\\train.json\n",
      "‚úÖ Saved CSV to data\\raw\\train.csv\n",
      "üìä TRAIN Split Info:\n",
      "   - Rows: 62\n",
      "   - Columns: ['guid', 'title', 'abstract', 'article']\n",
      "   - Sample data:\n",
      "     guid: 1\n",
      "     title: Kh·ªüi_t·ªë k·∫ª_tr·ªôm h∆°n 1 t·∫° th√≥c v√† h∆°n 8 tri·ªáu ƒë·ªìng c·ªßa ch√∫ ru·ªôt ƒë·ªÉ l·∫•y ti·ªÅn mua ma_tu√Ω\n",
      "     abstract: V·ªõi b·∫£n_t√≠nh ham ch∆°i , l∆∞·ªùi l√†m , c√≥ nhi·ªÅu ti·ªÅn_√°n ti·ªÅn_s·ª± , l·∫°i nghi·ªán ma_tu√Ω , Th∆∞∆°ng ƒë√£ ƒë·ªôt_nh·∫≠p...\n",
      "     article: Ng√†y 27/3 , C∆°_quan C·∫£nh_s√°t ƒëi·ªÅu_tra C√¥ng_an TP. H∆∞ng_Y√™n , t·ªânh H∆∞ng_Y√™n cho bi·∫øt , ƒë∆°n_v·ªã v·ª´a ra ...\n",
      "\n",
      "--- Downloading VALIDATION split ---\n",
      "Starting paginated download for validation split...\n",
      "Fetching rows 0 to 100...\n",
      "Total rows collected: 100\n",
      "Fetching rows 100 to 200...\n",
      "Total rows collected: 200\n",
      "Fetching rows 200 to 300...\n",
      "Total rows collected: 300\n",
      "Fetching rows 300 to 400...\n",
      "Total rows collected: 400\n",
      "Fetching rows 400 to 500...\n",
      "Total rows collected: 500\n",
      "Fetching rows 500 to 600...\n",
      "Total rows collected: 600\n",
      "Fetching rows 600 to 700...\n",
      "Total rows collected: 700\n",
      "Fetching rows 700 to 800...\n",
      "Total rows collected: 800\n",
      "Fetching rows 800 to 900...\n",
      "Total rows collected: 900\n",
      "Fetching rows 900 to 1000...\n",
      "Total rows collected: 1000\n",
      "Fetching rows 1000 to 1100...\n",
      "Total rows collected: 1100\n",
      "Fetching rows 1100 to 1200...\n",
      "Total rows collected: 1200\n",
      "Fetching rows 1200 to 1300...\n",
      "Total rows collected: 1300\n",
      "Fetching rows 1300 to 1400...\n",
      "Total rows collected: 1400\n",
      "Fetching rows 1400 to 1500...\n",
      "Total rows collected: 1500\n",
      "Fetching rows 1500 to 1600...\n",
      "Total rows collected: 1600\n",
      "Fetching rows 1600 to 1700...\n",
      "Total rows collected: 1700\n",
      "Fetching rows 1700 to 1800...\n",
      "Total rows collected: 1800\n",
      "Fetching rows 1800 to 1900...\n",
      "Total rows collected: 1900\n",
      "Fetching rows 1900 to 2000...\n",
      "Total rows collected: 2000\n",
      "Fetching rows 2000 to 2100...\n",
      "Total rows collected: 2100\n",
      "Fetching rows 2100 to 2200...\n",
      "Total rows collected: 2200\n",
      "Fetching rows 2200 to 2300...\n",
      "Total rows collected: 2300\n",
      "Fetching rows 2300 to 2400...\n",
      "Total rows collected: 2400\n",
      "Fetching rows 2400 to 2500...\n",
      "Total rows collected: 2500\n",
      "Fetching rows 2500 to 2600...\n",
      "Total rows collected: 2600\n",
      "Fetching rows 2600 to 2700...\n",
      "Total rows collected: 2700\n",
      "Fetching rows 2700 to 2800...\n",
      "Total rows collected: 2800\n",
      "Fetching rows 2800 to 2900...\n",
      "Total rows collected: 2900\n",
      "Fetching rows 2900 to 3000...\n",
      "Total rows collected: 3000\n",
      "Fetching rows 3000 to 3100...\n",
      "Total rows collected: 3100\n",
      "Fetching rows 3100 to 3200...\n",
      "Total rows collected: 3200\n",
      "Fetching rows 3200 to 3300...\n",
      "Total rows collected: 3300\n",
      "Fetching rows 3300 to 3400...\n",
      "Total rows collected: 3400\n",
      "Fetching rows 3400 to 3500...\n",
      "Total rows collected: 3500\n",
      "Fetching rows 3500 to 3600...\n",
      "Total rows collected: 3600\n",
      "Fetching rows 3600 to 3700...\n",
      "Total rows collected: 3700\n",
      "Fetching rows 3700 to 3800...\n",
      "Total rows collected: 3800\n",
      "Fetching rows 3800 to 3900...\n",
      "Total rows collected: 3900\n",
      "Fetching rows 3900 to 4000...\n",
      "Total rows collected: 4000\n",
      "Fetching rows 4000 to 4100...\n",
      "Total rows collected: 4100\n",
      "Fetching rows 4100 to 4200...\n",
      "Total rows collected: 4200\n",
      "Fetching rows 4200 to 4300...\n",
      "Total rows collected: 4300\n",
      "Fetching rows 4300 to 4400...\n",
      "Total rows collected: 4400\n",
      "Fetching rows 4400 to 4500...\n",
      "Total rows collected: 4500\n",
      "Fetching rows 4500 to 4600...\n",
      "Total rows collected: 4600\n",
      "Fetching rows 4600 to 4700...\n",
      "Total rows collected: 4700\n",
      "Fetching rows 4700 to 4800...\n",
      "Total rows collected: 4800\n",
      "Fetching rows 4800 to 4900...\n",
      "Total rows collected: 4900\n",
      "Fetching rows 4900 to 5000...\n",
      "Total rows collected: 5000\n",
      "Fetching rows 5000 to 5100...\n",
      "Total rows collected: 5100\n",
      "Fetching rows 5100 to 5200...\n",
      "Total rows collected: 5200\n",
      "Fetching rows 5200 to 5300...\n",
      "Total rows collected: 5300\n",
      "Fetching rows 5300 to 5400...\n",
      "Total rows collected: 5400\n",
      "Fetching rows 5400 to 5500...\n",
      "Total rows collected: 5500\n",
      "Fetching rows 5500 to 5600...\n",
      "Total rows collected: 5600\n",
      "Fetching rows 5600 to 5700...\n",
      "Total rows collected: 5700\n",
      "Fetching rows 5700 to 5800...\n",
      "Total rows collected: 5800\n",
      "Fetching rows 5800 to 5900...\n",
      "Total rows collected: 5900\n",
      "Fetching rows 5900 to 6000...\n",
      "Total rows collected: 6000\n",
      "Fetching rows 6000 to 6100...\n",
      "Total rows collected: 6100\n",
      "Fetching rows 6100 to 6200...\n",
      "Total rows collected: 6200\n",
      "Fetching rows 6200 to 6300...\n",
      "Total rows collected: 6300\n",
      "Fetching rows 6300 to 6400...\n",
      "Total rows collected: 6400\n",
      "Fetching rows 6400 to 6500...\n",
      "Total rows collected: 6500\n",
      "Fetching rows 6500 to 6600...\n",
      "Total rows collected: 6600\n",
      "Fetching rows 6600 to 6700...\n",
      "‚ùå Error during paginated download: 429 Client Error: Too Many Requests for url: https://datasets-server.huggingface.co/rows?dataset=ithieund%2FVietNews-Abs-Sum&config=default&split=validation&offset=6600&length=100\n",
      "Falling back to first-rows for validation...\n",
      "Fetching first 100 rows from validation split...\n",
      "‚úÖ Retrieved 62 rows from validation\n",
      "‚úÖ Saved 62 rows to data\\raw\\validation.json\n",
      "‚úÖ Saved CSV to data\\raw\\validation.csv\n",
      "üìä VALIDATION Split Info:\n",
      "   - Rows: 62\n",
      "   - Columns: ['guid', 'title', 'abstract', 'article']\n",
      "   - Sample data:\n",
      "     guid: 1\n",
      "     title: Kh·ªüi_t·ªë , t·∫°m giam hai nh√¢n_vi√™n c√¥ng_ty ƒê·ªãa_·ªëc Alibaba ch·ªëng l·ªánh c∆∞·ª°ng_ch·∫ø\n",
      "     abstract: Vi·ªán_ki·ªÉm_s√°t nh√¢n_d√¢n T X.Ph√∫ M·ªπ , t·ªânh B√†_R·ªãa ‚Äì V≈©ng_T√†u v·ª´a ph√™_chu·∫©n quy·∫øt_ƒë·ªãnh kh·ªüi_t·ªë b·ªã_can ,...\n",
      "     article: Ng√†y 22/6 , c∆°_quan CSƒêT C√¥ng_an T X.Ph√∫ M·ªπ ƒë√£ ra quy·∫øt_ƒë·ªãnh kh·ªüi_t·ªë v·ª• √°n h√¨nh_s·ª± v·ªÅ c√°c t·ªôi G√¢y_r·ªë...\n",
      "\n",
      "--- Downloading TEST split ---\n",
      "Starting paginated download for test split...\n",
      "Fetching rows 0 to 100...\n",
      "‚ùå Error during paginated download: 429 Client Error: Too Many Requests for url: https://datasets-server.huggingface.co/rows?dataset=ithieund%2FVietNews-Abs-Sum&config=default&split=test&offset=0&length=100\n",
      "Falling back to first-rows for test...\n",
      "Fetching first 100 rows from test split...\n",
      "‚úÖ Retrieved 57 rows from test\n",
      "‚úÖ Saved 57 rows to data\\raw\\test.json\n",
      "‚úÖ Saved CSV to data\\raw\\test.csv\n",
      "üìä TEST Split Info:\n",
      "   - Rows: 57\n",
      "   - Columns: ['guid', 'title', 'abstract', 'article']\n",
      "   - Sample data:\n",
      "     guid: 1\n",
      "     title: B·∫£n_√°n cho ƒë·ªëi_t∆∞·ª£ng gi·∫£_danh c√¥ng_an ƒë·ªÉ l·ª´a_ƒë·∫£o\n",
      "     abstract: Ng√†y 25/2 , TAND TP. ƒê√†_N·∫µng tuy√™n_ph·∫°t H·ªì_Xu√¢n_Huy ( SN 1994 ) , ng·ª• qu·∫≠n H·∫£i_Ch√¢u , 12 nƒÉm t√π v·ªÅ t...\n",
      "     article: Theo l·ªùi khai c·ªßa Huy t·∫°i phi√™n_to√† , ƒë·ªÉ c√≥ ti·ªÅn s·ª≠_d·ª•ng c√°_nh√¢n , Huy \" n·ªï \" l√† sƒ©_quan c·ª•c Ph√≤ng_c...\n",
      "\n",
      "4. Download Summary:\n",
      "------------------------------\n",
      "üìÅ dataset_info.json (3.3 KB)\n",
      "üìÅ test.csv (190.4 KB)\n",
      "üìÅ test.json (194.5 KB)\n",
      "üìÅ train.csv (188.8 KB)\n",
      "üìÅ train.json (193.3 KB)\n",
      "üìÅ validation.csv (189.7 KB)\n",
      "üìÅ validation.json (194.1 KB)\n",
      "\n",
      "‚úÖ Download completed!\n",
      "üìÇ Data location: data\n"
     ]
    }
   ],
   "source": [
    "print(\"Choose download option:\")\n",
    "print(\"1. Quick test (1000 rows per split)\")\n",
    "print(\"2. Full download (all data)\")\n",
    "    \n",
    "choice = input(\"Enter choice (1 or 2): \").strip()\n",
    "    \n",
    "if choice == \"1\":\n",
    "    quick_download()\n",
    "elif choice == \"2\":\n",
    "    full_download()\n",
    "else:\n",
    "    print(\"Invalid choice. Running quick test...\")\n",
    "    quick_download()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
